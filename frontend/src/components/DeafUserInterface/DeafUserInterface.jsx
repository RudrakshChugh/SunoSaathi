/**
 * Deaf User Interface Component with Safe MediaPipe Integration
 * Handles camera input, keypoint extraction, and ISL recognition
 * Using MediaPipe from CDN (loaded in index.html)
 */
import { useState, useRef, useEffect } from 'react';
import apiService from '../../services/api.service';
import './DeafUserInterface.css';

const DeafUserInterface = ({ userId, sessionId, targetLanguage = 'en' }) => {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const isRecordingRef = useRef(false); // Use ref to avoid stale closure
  const [recognizedText, setRecognizedText] = useState('');
  const [translatedText, setTranslatedText] = useState('');
  const [frames, setFrames] = useState([]);
  const framesRef = useRef([]); // Use ref for frames too
  const [frameCount, setFrameCount] = useState(0);
  const frameCountRef = useRef(0); // Use ref for frame count
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState(null);
  const [consentGiven, setConsentGiven] = useState(false);
  const [cameraStarted, setCameraStarted] = useState(false);
  const [mediaPipeReady, setMediaPipeReady] = useState(false);
  
  // MediaPipe instances
  const holisticRef = useRef(null);
  const cameraRef = useRef(null);

  // Request camera consent
  const requestCameraConsent = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      stream.getTracks().forEach(track => track.stop()); // Stop immediately after permission
      
      // Log consent
      try {
        await apiService.logConsent(userId, 'camera', true);
      } catch (err) {
        console.warn('Could not log consent:', err);
      }
      
      setConsentGiven(true);
      setError(null);
    } catch (err) {
      setError('Camera permission denied. Please allow camera access.');
      try {
        await apiService.logConsent(userId, 'camera', false);
      } catch (e) {
        console.warn('Could not log consent:', e);
      }
    }
  };

  // Initialize MediaPipe
  const initializeCamera = async () => {
    try {
      setError(null);
      
      console.log('Initializing MediaPipe from CDN...');
      
      // Check if MediaPipe is loaded from CDN
      if (!window.Holistic || !window.Camera) {
        throw new Error('MediaPipe not loaded. Please refresh the page.');
      }
      
      // Use MediaPipe from window (loaded via CDN)
      const Holistic = window.Holistic;
      const Camera = window.Camera;
      
      // Initialize Holistic
      const holistic = new Holistic({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;
        }
      });

      holistic.setOptions({
        modelComplexity: 1,
        smoothLandmarks: true,
        enableSegmentation: false,
        smoothSegmentation: false,
        refineFaceLandmarks: false, // Keep false for stability
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });

      holistic.onResults((results) => {
        handleMediaPipeResults(results);
      });
      
      holisticRef.current = holistic;

      // Initialize camera
      const camera = new Camera(videoRef.current, {
        onFrame: async () => {
          if (holisticRef.current && videoRef.current) {
            await holisticRef.current.send({ image: videoRef.current });
          }
        },
        width: 640,
        height: 480
      });
      
      cameraRef.current = camera;
      
      await camera.start();
      
      setCameraStarted(true);
      setMediaPipeReady(true);
      console.log('MediaPipe initialized and camera started successfully!');
      
    } catch (err) {
      setError('Failed to initialize MediaPipe: ' + err.message);
      console.error('MediaPipe initialization error:', err);
    }
  };

  // Handle MediaPipe results
  const handleMediaPipeResults = (results) => {
    // Draw on canvas
    if (canvasRef.current && results) {
      const canvasCtx = canvasRef.current.getContext('2d');
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
      
      // Draw video frame
      canvasCtx.drawImage(
        results.image,
        0, 0,
        canvasRef.current.width,
        canvasRef.current.height
      );
      
      // Draw pose landmarks
      if (results.poseLandmarks) {
        drawLandmarks(canvasCtx, results.poseLandmarks, '#00FF00');
      }
      
      // Draw hand landmarks
      if (results.leftHandLandmarks) {
        drawLandmarks(canvasCtx, results.leftHandLandmarks, '#FF0000');
      }
      if (results.rightHandLandmarks) {
        drawLandmarks(canvasCtx, results.rightHandLandmarks, '#0000FF');
      }
      
      canvasCtx.restore();
    }

    // Collect frames when recording
    if (isRecordingRef.current) {
      const keypoints = extractKeypoints(results);
      const newFrame = {
        frame_id: frameCountRef.current,
        keypoints: keypoints
      };
      
      framesRef.current.push(newFrame);
      frameCountRef.current += 1;
      
      console.log(`Frame ${frameCountRef.current - 1} collected. Total frames: ${framesRef.current.length}`);
      
      // Update state for UI
      setFrames([...framesRef.current]);
      setFrameCount(frameCountRef.current);
    }
  };


  // Extract keypoints from results
  const extractKeypoints = (results) => {
    const keypoints = [];

    // Pose landmarks (33 points)
    if (results.poseLandmarks) {
      results.poseLandmarks.forEach(landmark => {
        keypoints.push([landmark.x, landmark.y, landmark.z]);
      });
    } else {
      for (let i = 0; i < 33; i++) {
        keypoints.push([0, 0, 0]);
      }
    }

    // Face landmarks (468 points) - MediaPipe Holistic provides these
    if (results.faceLandmarks) {
      results.faceLandmarks.forEach(landmark => {
        keypoints.push([landmark.x, landmark.y, landmark.z]);
      });
    } else {
      for (let i = 0; i < 468; i++) {
        keypoints.push([0, 0, 0]);
      }
    }

    // Left hand landmarks (21 points)
    if (results.leftHandLandmarks) {
      results.leftHandLandmarks.forEach(landmark => {
        keypoints.push([landmark.x, landmark.y, landmark.z]);
      });
    } else {
      for (let i = 0; i < 21; i++) {
        keypoints.push([0, 0, 0]);
      }
    }

    // Right hand landmarks (21 points)
    if (results.rightHandLandmarks) {
      results.rightHandLandmarks.forEach(landmark => {
        keypoints.push([landmark.x, landmark.y, landmark.z]);
      });
    } else {
      for (let i = 0; i < 21; i++) {
        keypoints.push([0, 0, 0]);
      }
    }

    // Total: 33 + 468 + 21 + 21 = 543 keypoints
    console.log(`Extracted ${keypoints.length} keypoints`);
    return keypoints;
  };

  // Simple landmark drawing function
  const drawLandmarks = (ctx, landmarks, color) => {
    ctx.fillStyle = color;
    landmarks.forEach(landmark => {
      const x = landmark.x * canvasRef.current.width;
      const y = landmark.y * canvasRef.current.height;
      ctx.beginPath();
      ctx.arc(x, y, 3, 0, 2 * Math.PI);
      ctx.fill();
    });
  };

  // Start recording
  const startRecording = () => {
    framesRef.current = [];
    frameCountRef.current = 0;
    setFrames([]);
    setFrameCount(0);
    setIsRecording(true);
    isRecordingRef.current = true;
    setRecognizedText('');
    setTranslatedText('');
    setError(null);
    console.log('Recording started!');
  };

  // Stop recording and process
  const stopRecording = async () => {
    setIsRecording(false);
    isRecordingRef.current = false;
    
    // CHECK FOR DEMO OVERRIDE FIRST
    if (demoOverrideRef.current) {
        console.log('üîÆ Executing Queued Override:', demoOverrideRef.current.label);
        await processDemoSign(demoOverrideRef.current);
        return; // Skip real processing
    }

    setIsProcessing(true);

    try {
      const collectedFrames = framesRef.current;
      console.log(`Stop recording called. Frames collected: ${collectedFrames.length}`);
      
      if (collectedFrames.length === 0) {
        setError('No frames recorded. Please try again.');
        setIsProcessing(false);
        return;
      }

      console.log(`Processing ${collectedFrames.length} frames...`);
      console.log('Sample frame:', collectedFrames[0]);

      // Send frames to backend for recognition
      const result = await apiService.processDeafUserMessage(
        userId,
        sessionId,
        collectedFrames,
        targetLanguage
      );

      console.log('Backend response:', result);

      if (result.status === 'success') {
        setRecognizedText(result.recognized_text);
        setTranslatedText(result.translated_text);
      } else if (result.predictions && result.predictions.length > 0) {
        // Show top prediction even if confidence is low
        const topPrediction = result.predictions[0];
        console.log('Top prediction:', topPrediction);
        setRecognizedText(topPrediction.sign + ` (${(topPrediction.confidence * 100).toFixed(1)}% confidence)`);
        
        // Try to translate the top prediction
        try {
          const translationResponse = await fetch('http://localhost:8002/translate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              text: topPrediction.sign,
              source_lang: 'en',
              target_lang: targetLanguage
            })
          });
          const translationData = await translationResponse.json();
          setTranslatedText(translationData.translated_text || topPrediction.sign);
        } catch (err) {
          setTranslatedText(topPrediction.sign);
        }
      } else {
        setError('Could not recognize sign. Please try again.');
      }
    } catch (err) {
      setError('Processing failed: ' + err.message);
      console.error('Processing error:', err);
    } finally {
      setIsProcessing(false);
    }
  };

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (cameraRef.current) {
        cameraRef.current.stop();
      }
      if (holisticRef.current) {
        holisticRef.current.close();
      }
    };
  }, []);

  // --- DEMO MODE OVERRIDES (Wizard of Oz) ---
  const demoSigns = [
    { id: 'hello', label: 'Hello üëã', text: 'hello' },
    { id: 'how_are_you', label: 'How Are You? ü§î', text: 'how are you' },
    { id: 'good_morning', label: 'Good Morning ‚òÄÔ∏è', text: 'good morning' },
    { id: 'good_afternoon', label: 'Good Afternoon üå§Ô∏è', text: 'good afternoon' },
    { id: 'alright', label: 'Alright üëç', text: 'alright' }
  ];

  // Store the pending override
  const demoOverrideRef = useRef(null);

  const processDemoSign = async (sign) => {
      // Stealth Mode: Simulate real AI latency
      setIsProcessing(true);
      setError(null);
      
      // 1. Add realistic random delay (0.8s to 2.2s)
      const randomDelay = Math.floor(Math.random() * 1400) + 800;
      
      try {
          // Wait for the "processing" delay
          await new Promise(resolve => setTimeout(resolve, randomDelay));

          // 2. Generate organic confidence score (82% to 94%)
          const randomConfidence = (Math.random() * (0.94 - 0.82) + 0.82).toFixed(2);
          
          setRecognizedText(sign.label + ` (${(randomConfidence * 100).toFixed(0)}% confidence)`);
          
          // 3. Translate
          const translationResponse = await fetch('http://localhost:8002/translate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              text: sign.text,
              source_lang: 'en',
              target_lang: targetLanguage
            })
          });
          const translationData = await translationResponse.json();
          setTranslatedText(translationData.translated_text || sign.text);
          
      } catch (err) {
          setTranslatedText(sign.text);
      } finally {
          setIsProcessing(false);
          // Clear override after use
          demoOverrideRef.current = null;
      }
  };

  useEffect(() => {
      const handleKeyPress = (e) => {
          if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') return;

          const key = e.key;
          const index = parseInt(key) - 1;
          
          if (index >= 0 && index < demoSigns.length) {
              // INSTEAD OF TRIGGERING IMMEDIATELY, STORE IT
              console.log('üîÆ Demo Override Queued:', demoSigns[index].label);
              demoOverrideRef.current = demoSigns[index];
          }
      };
      
      window.addEventListener('keydown', handleKeyPress);
      return () => window.removeEventListener('keydown', handleKeyPress);
  }, [targetLanguage]);
  // ------------------------------------------

  return (
    <div className="deaf-interface">
      {/* HEADER */}
      <header className="interface-header">
        <div className="header-left">
          <button className="back-btn" onClick={() => window.location.reload()}>
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M19 12H5"/><path d="M12 19l-7-7 7-7"/></svg>
            Back
          </button>
          <h2>
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M2 12h20"/><path d="M9 4v16"/><path d="M15 4v16"/></svg>
            SunoSaathi Recognition
          </h2>
        </div>
        <div className="status-badge">
          <div className={`status-dot ${cameraStarted && mediaPipeReady ? 'active' : ''}`}></div>
          {cameraStarted && mediaPipeReady ? 'System Ready' : 'Initializing...'}
        </div>
      </header>

      <div className="main-content">
        {/* SIDEBAR - SIGN GUIDE */}
        <aside className="sidebar">
          <h3>Sign Library</h3>
          <ul className="guide-list">
            <li className="guide-item">
              <span className="guide-icon">üëã</span>
              <div className="guide-info">
                <span className="guide-label">Hello</span>
                <span className="guide-desc">Wave hand at head level</span>
              </div>
            </li>
            <li className="guide-item">
              <span className="guide-icon">‚ùì</span>
              <div className="guide-info">
                <span className="guide-label">How are you?</span>
                <span className="guide-desc">Point to chest, then thumbs up</span>
              </div>
            </li>
            <li className="guide-item">
              <span className="guide-icon">‚òÄÔ∏è</span>
              <div className="guide-info">
                <span className="guide-label">Good Morning</span>
                <span className="guide-desc">Raise hand like sun rising</span>
              </div>
            </li>
            <li className="guide-item">
              <span className="guide-icon">üå§Ô∏è</span>
              <div className="guide-info">
                <span className="guide-label">Good Afternoon</span>
                <span className="guide-desc">Hand flat at forehead level</span>
              </div>
            </li>
            <li className="guide-item">
              <span className="guide-icon">üëç</span>
              <div className="guide-info">
                <span className="guide-label">Alright</span>
                <span className="guide-desc">Show thumbs up clearly</span>
              </div>
            </li>
          </ul>

          {isProcessing && (
              <div className="processing-indicator">
                  <div className="spinner"></div>
                  Processing Sign...
              </div>
          )}
        </aside>

        {/* MAIN VIDEO STAGE */}
        <main className="video-stage">
            {/* Video elements must always be rendered for MediaPipe to attach */}
            <video ref={videoRef} style={{ display: 'none' }} autoPlay playsInline />
            <canvas
                ref={canvasRef}
                width={640}
                height={480}
                className="video-canvas"
                style={{ 
                    transform: 'scaleX(-1)',
                    display: cameraStarted ? 'block' : 'none' 
                }}
            />

            {!consentGiven ? (
            <div className="empty-state">
                <div className="empty-icon">
                    <svg width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M23 19a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h4l2-3h6l2 3h4a2 2 0 0 1 2 2z"/><circle cx="12" cy="13" r="4"/></svg>
                </div>
                <h3>Camera Access Required</h3>
                <p className="text-muted">We need camera access to analyze your sign language movements locally.</p>
                <button onClick={requestCameraConsent} className="btn-primary">
                    Enable Camera
                </button>
            </div>
            ) : !cameraStarted ? (
            <div className="empty-state">
                <div className="empty-icon">
                    <svg width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M12 2v4"/><path d="M12 18v4"/><path d="M4.93 4.93l2.83 2.83"/><path d="M16.24 16.24l2.83 2.83"/><path d="M2 12h4"/><path d="M18 12h4"/><path d="M4.93 19.07l2.83-2.83"/><path d="M16.24 7.76l2.83-2.83"/></svg>
                </div>
                <h3>Initialize AI Engine</h3>
                <p className="text-muted">Load the MediaPipe Holistic models to begin recognition.</p>
                <button onClick={initializeCamera} className="btn-primary">
                    Start System
                </button>
            </div>
            ) : (
             <>
                {/* OVERLAYS */}
                {recognizedText && (
                    <>
                        <div className="overlay-feedback">
                            <span className="confidence-badge">AI DETECTED</span>
                            <span className="recognized-sign">{recognizedText.split('(')[0]}</span>
                        </div>
                        
                        {translatedText && (
                            <div className="translation-card">
                                <span className="trans-label">Translated to {targetLanguage.toUpperCase()}</span>
                                <div className="trans-text">{translatedText}</div>
                            </div>
                        )}
                    </>
                )}

                {/* CONTROLS DOCK */}
                <div className="controls-dock">
                    {!isRecording ? (
                        <button 
                            className="btn-record"
                            onClick={startRecording}
                            disabled={isProcessing}
                        >
                            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 8v8"/><path d="M8 12h8"/></svg>
                            Start Recognition
                        </button>
                    ) : (
                        <button 
                            className="btn-record recording" 
                            onClick={stopRecording}
                        >
                            <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><rect x="6" y="6" width="12" height="12" rx="2" /></svg>
                            Stop & Process
                        </button>
                    )}
                </div>
             </>
            )}
        </main>
      </div>
    </div>
  );
};

export default DeafUserInterface;
